overall objective/outline:
>1500words
format example at jmlr.org 
use this as format https://media.neurips.cc/Conferences/NeurIPS2023/Styles/neurips_2023.pdf
abstract, introduction, method, experiment, conclusion, references

grading criteria: 
"Interestingness"  (10 points).
dataset size and difficulty (10 points)
new aspects in algo/design/application (20 points)

what do:
find datasets
create research question
how to connect these 2 ^

use some method (prob char rnn) to give some solution based on data


options:
train CNN on some network structure, similar to prev group project
char rnn- generates by character
          use https://github.com/jcjohnson/torch-rnn for faster model


Topic ideas:
char rnn english to kannada
char rnn relating to some game terms?
summarize a book, pick context for summarization if possible



char rnn:
sample.lua https://github.com/jcjohnson/torch-rnn/blob/master/sample.lua line 10: start text
https://huggingface.co/datasets/kannada_news kannada dataset






model is an lstm




done till now:

say that generation wasnt as good when all cols done
training was too slow with very large dataset


downloaded sample code
downloaded container, opened
created scripts to make opening things easier
downloaded dataset
preprocessed to make text ok

mention parameters: batch size, seq length, iterations, epocks, file size, dataset, loss

find paper about rnn text generation

commands:

python scripts/preprocess.py --input_txt data/random_1000_lyrics.txt --output_h5 data/spotify_output.h5 --output_json data/spotify_output.json
docker cp data/random_1000_lyrics.txt 6c35f25d0a5f:/root/torch-rnn/data/
docker cp 6c35f25d0a5f:/root/torch-rnn/cv/checkpoint_3000.json cv/
th train.lua -input_h5 data/spotify_output.h5 -input_json data/spotify_output.json


optimizations:
increased batch size
increased learning rate
reduced max epochs
more frequent checkpoints!


todo: 
figure out to use eval.lua: gives loss percentage etc on a given checkpoint
update commands
say that my model is doing better than other options baseed on xyz testing benchmarks/methodologies
init_from in train.lua needs checkpoint if we want to initialize from it, give cv/checkpointname
bs about experimental design and tuning